{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x OPTDecoderLayer(\n",
       "          (self_attn): OPTSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from datamodule import WikiTextV2Datamodule\n",
    "import os\n",
    "from constants import TARGET_MODEL, DRAFT_MODEL\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = \"cuda:4\"\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\").to(device)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "target_model.eval()\n",
    "draft_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучиться на 1 батче:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amirelkanov/Fabula/.venv/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27268749475479126\n",
      "0.12750867009162903\n",
      "0.05827204883098602\n",
      "0.047447118908166885\n",
      "0.03230045363306999\n",
      "0.02384386770427227\n",
      "0.020718352869153023\n",
      "0.016534434631466866\n",
      "0.014508516527712345\n",
      "0.009837430901825428\n",
      "0.011079397983849049\n",
      "0.013059280812740326\n",
      "0.009622174315154552\n",
      "0.01495021115988493\n",
      "0.009480783715844154\n",
      "0.011719456873834133\n",
      "0.017221765592694283\n",
      "0.009780880995094776\n",
      "0.010229299776256084\n",
      "0.008598558604717255\n",
      "0.01047136727720499\n",
      "0.008495763875544071\n",
      "0.010267514735460281\n",
      "0.00801050290465355\n",
      "0.009800273925065994\n",
      "0.012494172900915146\n",
      "0.011017469689249992\n",
      "0.00961147528141737\n",
      "0.01980660855770111\n",
      "0.011592146009206772\n",
      "0.009667702950537205\n",
      "0.01057096105068922\n",
      "0.015266941860318184\n",
      "0.010866181924939156\n",
      "0.01014289353042841\n",
      "0.01787448860704899\n",
      "0.009766926057636738\n",
      "0.01346069946885109\n",
      "0.009608574211597443\n",
      "0.010621866211295128\n",
      "0.010807722806930542\n",
      "0.006432997062802315\n",
      "0.009688368067145348\n",
      "0.008376557379961014\n",
      "0.007480780594050884\n",
      "0.007161600515246391\n",
      "0.011216191574931145\n",
      "0.015565186738967896\n",
      "0.006254633888602257\n",
      "0.009590625762939453\n",
      "0.009142170660197735\n",
      "0.013129111379384995\n",
      "0.008889785036444664\n",
      "0.008079865016043186\n",
      "0.009398242458701134\n",
      "0.011742902919650078\n",
      "0.015921156853437424\n",
      "0.012137974612414837\n",
      "0.015549322590231895\n",
      "0.020660005509853363\n",
      "0.011120451614260674\n",
      "0.013779781758785248\n",
      "0.009504584595561028\n",
      "0.010373679921030998\n",
      "0.006828625686466694\n",
      "0.006333382334560156\n",
      "0.006472532171756029\n",
      "0.008328605443239212\n",
      "0.004413994960486889\n",
      "0.006569260731339455\n",
      "0.008069077506661415\n",
      "0.006404740270227194\n",
      "0.009325853548943996\n",
      "0.008726129308342934\n",
      "0.006293171085417271\n",
      "0.008978774771094322\n",
      "0.0074472324922680855\n",
      "0.007051725406199694\n",
      "0.006583489011973143\n",
      "0.007857797667384148\n",
      "0.010057132691144943\n",
      "0.004546090494841337\n",
      "0.009510103613138199\n",
      "0.007343273144215345\n",
      "0.01995883136987686\n",
      "0.007872741669416428\n",
      "0.009543154388666153\n",
      "0.006058120168745518\n",
      "0.007748480886220932\n",
      "0.010647057555615902\n",
      "0.011507595889270306\n",
      "0.007981355302035809\n",
      "0.005904753226786852\n",
      "0.005653980188071728\n",
      "0.008090446703135967\n",
      "0.008652222342789173\n",
      "0.006614296697080135\n",
      "0.00697952788323164\n",
      "0.009679872542619705\n",
      "0.010056156665086746\n",
      "0.007494065910577774\n",
      "0.00754086347296834\n",
      "0.007333182729780674\n",
      "0.006666352041065693\n",
      "0.008062888868153095\n",
      "0.0032925307750701904\n",
      "0.003592927008867264\n",
      "0.00521087134256959\n",
      "0.007278867997229099\n",
      "0.010031072422862053\n",
      "0.004148822743445635\n",
      "0.004856801591813564\n",
      "0.004192952066659927\n",
      "0.0051763951778411865\n",
      "0.0046536363661289215\n",
      "0.004849041812121868\n",
      "0.010417794808745384\n",
      "0.00328659825026989\n",
      "0.004503163509070873\n",
      "0.00462711276486516\n",
      "0.0071800462901592255\n",
      "0.00391240231692791\n",
      "0.0037727064918726683\n",
      "0.00455124955624342\n",
      "0.004864450078457594\n",
      "0.0046722013503313065\n",
      "0.006778656039386988\n",
      "0.003968436270952225\n",
      "0.004881331697106361\n",
      "0.004156095441430807\n",
      "0.0033219538163393736\n",
      "0.0074380505830049515\n",
      "0.003708974691107869\n",
      "0.005988901015371084\n",
      "0.0033389439340680838\n",
      "0.003120552748441696\n",
      "0.005372036248445511\n",
      "0.003119627945125103\n",
      "0.0055802143178880215\n",
      "0.004678405821323395\n",
      "0.005093962885439396\n",
      "0.003217550925910473\n",
      "0.005147834774106741\n",
      "0.005137795582413673\n",
      "0.005347816739231348\n",
      "0.006144838407635689\n",
      "0.007726940326392651\n",
      "0.006759923882782459\n",
      "0.004311360884457827\n",
      "0.0043526822701096535\n",
      "0.007892424240708351\n",
      "0.006764558143913746\n",
      "0.00694251386448741\n",
      "0.006553277373313904\n",
      "0.00420097541064024\n",
      "0.008460808545351028\n",
      "0.006260011810809374\n",
      "0.005917524918913841\n",
      "0.004136516246944666\n",
      "0.002616140292957425\n",
      "0.0063761696219444275\n",
      "0.006477653980255127\n",
      "0.006703875493258238\n",
      "0.005701405927538872\n",
      "0.006413932889699936\n",
      "0.006536458618938923\n",
      "0.004633150529116392\n",
      "0.0033550099469721317\n",
      "0.004340512678027153\n",
      "0.004060262814164162\n",
      "0.0028899151366204023\n",
      "0.0036133802495896816\n",
      "0.003688083030283451\n",
      "0.005021429155021906\n",
      "0.006029450334608555\n",
      "0.005123061127960682\n",
      "0.008096767589449883\n",
      "0.0033869605977088213\n",
      "0.007445541210472584\n",
      "0.008535001426935196\n",
      "0.00642703752964735\n",
      "0.005350767634809017\n",
      "0.005024641752243042\n",
      "0.005141856614500284\n",
      "0.005326523445546627\n",
      "0.0037231305614113808\n",
      "0.0052726720459759235\n",
      "0.006175634451210499\n",
      "0.0036502480506896973\n",
      "0.0031260130926966667\n",
      "0.0029385234229266644\n",
      "0.006251174490898848\n",
      "0.0039947303012013435\n",
      "0.005425422452390194\n",
      "0.008731788024306297\n",
      "0.005116977728903294\n",
      "0.002959722653031349\n",
      "0.006919287145137787\n",
      "0.0036888367030769587\n",
      "0.003642006777226925\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "datamodule = WikiTextV2Datamodule(\n",
    "    min_len=5,  \n",
    "    max_len=12,\n",
    "    target_model=target_model,\n",
    "    device=device,\n",
    "    batch_size=1, \n",
    ")\n",
    "datamodule.setup(stage=\"fit\")\n",
    "train_loader = datamodule.train_dataloader()\n",
    "optimizer = AdamW(\n",
    "    [p for p in draft_model.parameters() if p.requires_grad],\n",
    "    lr=0.001,\n",
    "    weight_decay=0.001\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "batch = next(iter(train_loader))\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    target_scores = batch[\"scores\"]\n",
    "    \n",
    "    draft_outputs = draft_model(input_ids)\n",
    "    draft_logits = draft_outputs.logits[:, -1, :]\n",
    "    \n",
    "    log_draft_probs = F.log_softmax(draft_logits, dim=-1)\n",
    "    target_probs = F.softmax(target_scores, dim=-1)    \n",
    "    \n",
    "    loss = F.kl_div(log_draft_probs, target_probs, reduction='batchmean')\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(loss.item())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type           | Params | Mode\n",
      "-------------------------------------------------------\n",
      "0 | draft_model  | OPTForCausalLM | 125 M  | eval\n",
      "1 | target_model | OPTForCausalLM | 1.3 B  | eval\n",
      "-------------------------------------------------------\n",
      "125 M     Trainable params\n",
      "1.3 B     Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,763.990 Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "412       Modules in eval mode\n",
      "/home/amirelkanov/Fabula/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bc0032a0724a4b89e9ccce9b4bee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "from finetune_draft_model import DraftModelFinetuner \n",
    "import lightning as L\n",
    "\n",
    "datamodule = WikiTextV2Datamodule(\n",
    "    min_len=5,  \n",
    "    max_len=12,\n",
    "    target_model=target_model,\n",
    "    device=device,\n",
    "    batch_size=1, \n",
    ")\n",
    "datamodule.setup(stage=\"fit\")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", max_epochs=3, limit_train_batches=None, logger=False, devices=[4] # TensorBoardLogger(save_dir=\".\")\n",
    ")\n",
    "\n",
    "finetuner = DraftModelFinetuner()\n",
    "trainer.fit(model=finetuner, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(finetuner.state_dict(), 'model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
